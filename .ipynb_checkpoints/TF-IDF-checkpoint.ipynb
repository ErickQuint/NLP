{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6865f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def minusculas(texto):\n",
    "    return texto.lower()\n",
    "\n",
    "def remover_puntuaciones(texto):\n",
    "    punctuaciones = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    \n",
    "    for c in texto:\n",
    "        if c in punctuaciones:\n",
    "            texto = texto.replace(c, '')\n",
    "    \n",
    "    return texto\n",
    "\n",
    "def tokenizar(texto):\n",
    "    return texto.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90689d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = [\"La inteligencia artificial (IA) actual funciona bien cuando la aplicas a un ámbito muy concreto: desde jugar al ajedrez a hacer un diagnóstico médico. Pero el gran reto en el siglo XXI, consiste en alcanzar una inteligencia artificial general (AGI por sus siglas en inglés), que es aquella IA capaz de aprender tareas intelectuales como lo hacen los humanos. En este contexto, destaca GPT-3, un modelo de IA que permite generar lenguaje escrito. Es lo que se conoce como un modelo de lenguaje auto-regresivo, es decir, 'un algoritmo que permite crear la siguiente mejor palabra que seguiría a un texto dado', explica César de Pablo, científico de datos en BBVA Data &Analytics.\",\n",
    "         \"Desde Yo, robot la humanidad ha vivido aterrada con la idea de que los robots cobren vida y consciencia propia. Puede que la inteligencia artificial nos ponga un paso más cerca de ese día, y según algunos eso ya habría ocurrido en los cuarteles de una de los gigantes tecnológicos más importantes del siglo XXI: Google. El programa LaMDA, una IA  especializada en conversaciones, pudo haber cobrado consciencia según uno de los ingenieros participantes en el proyecto. LaMDA (Language Model for Dialogue Applications, modelo de lenguaje para aplicaciones de diálogo en español) es un programa diseñado para tener conversaciones realistas con un ser humano prestando atención a 'un aparente sinnúmero de temas', como sucede normalmente en nuestras pláticas.\",\n",
    "         \"El francés Aspect, el estadounidense Clauser y el austriaco Zeilinger son tres pioneros de los revolucionarios mecanismos de la física cuántica. Han sido premiados por sus descubrimientos sobre el 'entrelazamiento cuántico', un fenómeno por el que dos partículas cuánticas están perfectamente correlacionadas, independientemente de la distancia que las separe, segñun ha anunciado el jurado del Nobel.\"]\n",
    "    \n",
    "total_documentos = len(textos)\n",
    "\n",
    "i = 0\n",
    "vocabulario = []\n",
    "\n",
    "for texto in textos:\n",
    "    texto_aux = minusculas(texto)\n",
    "    texto_aux = remover_puntuaciones(texto_aux)\n",
    "    texto_tokenizado = tokenizar(texto_aux)\n",
    "    \n",
    "    textos[i] = texto_tokenizado\n",
    "    \n",
    "    for palabra in textos[i]:\n",
    "        if palabra not in vocabulario:\n",
    "            vocabulario.append(palabra)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "diccionario_palabras = {} \n",
    "i = 0\n",
    "\n",
    "for palabra in vocabulario:\n",
    "    diccionario_palabras[palabra] = i\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d8ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    " def cuenta_palabra_documentos(textos):    \n",
    "    ocurrencias = {}\n",
    "    \n",
    "    for palabra in vocabulario:\n",
    "        ocurrencias[palabra] = 0\n",
    "        for texto in textos:\n",
    "            if palabra in texto:\n",
    "                ocurrencias[palabra] += 1\n",
    "                \n",
    "    return ocurrencias\n",
    "    \n",
    "ocurrencias = cuenta_palabra_documentos(textos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d971d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frecuencia_termino(texto, palabra):\n",
    "    n = len(texto)\n",
    "    ocurrencia = len([token for token in texto if token == palabra])\n",
    "    \n",
    "    return ocurrencia / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf95cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frecuencia_inversa_documentos(palabra):\n",
    "    try:\n",
    "        ocurrencia_palabra = ocurrencias[palabra] + 1\n",
    "    except:\n",
    "        ocurrencia_palabra = 1\n",
    "        \n",
    "    return np.log(total_documentos / (1+ocurrencia_palabra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51a17e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(texto):\n",
    "    tf_idf_vec = np.zeros((len(vocabulario),))\n",
    "    \n",
    "    for palabra in texto:\n",
    "        tf = frecuencia_termino(texto, palabra)\n",
    "        idf = frecuencia_inversa_documentos(palabra)\n",
    "         \n",
    "        valor = tf*idf\n",
    "        tf_idf_vec[diccionario_palabras[palabra]] = valor \n",
    "        \n",
    "    return tf_idf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af11a084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01368283 -0.00513718 -0.00513718 -0.00770577  0.          0.\n",
      "  0.          0.          0.         -0.00770577 -0.02736566  0.\n",
      "  0.          0.         -0.00256859  0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.00912189  0.\n",
      "  0.         -0.01284295 -0.00256859 -0.00256859  0.          0.\n",
      " -0.00256859  0.          0.         -0.00256859 -0.00256859  0.\n",
      "  0.         -0.02280472 -0.00770577  0.          0.         -0.02280472\n",
      "  0.          0.          0.         -0.00513718  0.          0.\n",
      " -0.00456094  0.          0.          0.          0.          0.\n",
      " -0.00513718  0.          0.         -0.00513718  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "vectores = []\n",
    "\n",
    "for texto in textos:\n",
    "    vec = tf_idf(texto)\n",
    "    vectores.append(vec)\n",
    "\n",
    "print(vectores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e7e11f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud entre texto 1 y texto 2 = 0.845\n",
      "Similitud entre texto 1 y texto 3 = 0.6018\n",
      "Similitud entre texto 2 y texto 3 = 0.6448\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    " \n",
    "def raiz_cuadrada(x): \n",
    "    return round(sqrt(sum([a * a for a in x])),4)\n",
    "\n",
    "def similitudCoseno(x, y):\n",
    "    #sim_cos = np.dot(a, b) / np.sqrt(np.dot(a, a) * np.dot(b, b))\n",
    "    numerador = sum(a * b for a, b in zip(x, y))\n",
    "    denominador = raiz_cuadrada(x) * raiz_cuadrada(y)\n",
    "    \n",
    "    sim_cos = round(numerador/float(denominador),4)\n",
    "    \n",
    "    return sim_cos\n",
    "\n",
    "print(f\"Similitud entre texto 1 y texto 2 = {similitudCoseno(vectores[0], vectores[1])}\")\n",
    "print(f\"Similitud entre texto 1 y texto 3 = {similitudCoseno(vectores[0], vectores[2])}\")\n",
    "print(f\"Similitud entre texto 2 y texto 3 = {similitudCoseno(vectores[1], vectores[2])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
